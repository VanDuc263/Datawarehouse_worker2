import pandas as pd
from datetime import datetime
import configparser
import json
import sys
from meta_logger.meta_logger import get_logger, upload_log_to_minio


def main():
    # ========================
    # üîß C·∫•u h√¨nh logger
    # ========================
    step_name = "data_quality"
    logger = get_logger("quality_checker", log_dir="logs", step_name=step_name)

    # ========================
    # üìò ƒê·ªçc file c·∫•u h√¨nh
    # ========================
    config = configparser.ConfigParser()
    config.read("config.ini")

    # ========================
    # ‚òÅÔ∏è C·∫•u h√¨nh MinIO
    # ========================
    MINIO_STORAGE_OPTIONS = {
        "key": config["MINIO"]["key"],
        "secret": config["MINIO"]["secret"],
        "client_kwargs": {"endpoint_url": config["MINIO"]["endpoint_url"]},
    }

    # ========================
    # üóÇÔ∏è ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu
    # ========================
    bucket = config["PATHS"]["staging_bucket"]
    folder = config["PATHS"]["staging_folder"]
    today = datetime.now().strftime("%Y-%m-%d")

    staging_path = f"s3://{bucket}/{folder}/{today}/clean_data.csv"
    report_path = f"s3://{bucket}/{folder}/{today}/data_quality_report.csv"

    logger.info(f"üîπ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {staging_path}")

    # ========================
    # üì• ƒê·ªçc d·ªØ li·ªáu t·ª´ MinIO
    # ========================
    try:
        df = pd.read_csv(staging_path, storage_options=MINIO_STORAGE_OPTIONS)
        logger.info(f"‚úÖ S·ªë d√≤ng d·ªØ li·ªáu staging: {len(df)}")
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ƒë·ªçc d·ªØ li·ªáu t·ª´ MinIO: {e}")
        sys.exit(1)

    # ========================
    # üß† Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
    # ========================
    report = {}

    # T·ªïng s·ªë d√≤ng, s·ªë c·ªôt
    report["num_rows"] = len(df)
    report["num_cols"] = len(df.columns)
    logger.info(f"üìä T·ªïng s·ªë d√≤ng: {report['num_rows']}, s·ªë c·ªôt: {report['num_cols']}")

    # Null count
    report["null_counts"] = df.isnull().sum().to_dict()
    null_summary = {k: v for k, v in report["null_counts"].items() if v > 0}
    if null_summary:
        logger.warning(f"‚ö†Ô∏è C√≥ gi√° tr·ªã null ·ªü c√°c c·ªôt: {json.dumps(null_summary, ensure_ascii=False)}")
    else:
        logger.info("‚úÖ Kh√¥ng c√≥ gi√° tr·ªã null n√†o trong d·ªØ li·ªáu.")

    # Duplicate rows
    report["duplicate_rows"] = df.duplicated().sum()
    if report["duplicate_rows"] > 0:
        logger.warning(f"‚ö†Ô∏è C√≥ {report['duplicate_rows']} d√≤ng tr√πng l·∫∑p.")
    else:
        logger.info("‚úÖ Kh√¥ng c√≥ d√≤ng tr√πng l·∫∑p.")

    # Ki·ªÉm tra gi√° h·ª£p l·ªá (n·∫øu c√≥ c·ªôt 'gi√°')
    if "gi√°" in df.columns:
        report["gi√°_min"] = df["gi√°"].min()
        report["gi√°_max"] = df["gi√°"].max()
        report["gi√°_invalid"] = df[df["gi√°"] <= 0].shape[0]
        logger.info(f"üí∞ Gi√° nh·ªè nh·∫•t: {report['gi√°_min']}, l·ªõn nh·∫•t: {report['gi√°_max']}")

        if report["gi√°_invalid"] > 0:
            logger.warning(f"‚ö†Ô∏è C√≥ {report['gi√°_invalid']} d√≤ng c√≥ gi√° kh√¥ng h·ª£p l·ªá (‚â§ 0).")
        else:
            logger.info("‚úÖ T·∫•t c·∫£ gi√° tr·ªã 'gi√°' ƒë·ªÅu h·ª£p l·ªá.")

    # ========================
    # üíæ L∆∞u b√°o c√°o v√†o MinIO
    # ========================
    try:
        pd.DataFrame([report]).to_csv(report_path, index=False, storage_options=MINIO_STORAGE_OPTIONS)
        logger.info(f"üéØ ƒê√£ l∆∞u b√°o c√°o Data Quality v√†o: {report_path}")
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi l∆∞u b√°o c√°o Data Quality v√†o MinIO: {e}")
        sys.exit(1)

    # ========================
    # ‚òÅÔ∏è Upload log l√™n MinIO
    # ========================
    upload_log_to_minio(logger.log_file, step_name=step_name)


if __name__ == "__main__":
    main()

