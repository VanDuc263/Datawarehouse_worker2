import pandas as pd
from datetime import datetime
import configparser
import json
import sys
from meta_logger.meta_logger import get_logger, upload_log_to_minio
import s3fs


def update_file_status(file_name, status, bucket, minio_opts):
    """C·∫≠p nh·∫≠t tr·∫°ng th√°i P1-P4 cho file tr√™n MinIO"""
    status_file = f"s3://{bucket}/file_status.csv"
    try:
        fs = s3fs.S3FileSystem(**minio_opts["client_kwargs"],
                               key=minio_opts["key"],
                               secret=minio_opts["secret"])

        if fs.exists(status_file):
            df_status = pd.read_csv(status_file, storage_options=minio_opts)
        else:
            df_status = pd.DataFrame(columns=["file_name", "status", "last_update"])

        now = datetime.now().isoformat()
        if file_name in df_status["file_name"].values:
            df_status.loc[df_status["file_name"] == file_name, ["status", "last_update"]] = [status, now]
        else:
            df_status = pd.concat([
                df_status,
                pd.DataFrame([[file_name, status, now]], columns=df_status.columns)
            ], ignore_index=True)

        df_status.to_csv(status_file, index=False, encoding="utf-8-sig", storage_options=minio_opts)

    except Exception as e:
        print(f"‚ùå L·ªói c·∫≠p nh·∫≠t file_status.csv: {e}")


def main():
    step_name = "data_quality"
    logger = get_logger("quality_checker", log_dir="logs", step_name=step_name)

    # --- ƒê·ªçc config ---
    config = configparser.ConfigParser()
    config.read("config.ini")

    MINIO_STORAGE_OPTIONS = {
        "key": config["MINIO"]["key"],
        "secret": config["MINIO"]["secret"],
        "client_kwargs": {"endpoint_url": config["MINIO"]["endpoint_url"]},
    }

    # --- ƒê∆∞·ªùng d·∫´n ---
    bucket = config["MINIO"]["bucket"]
    folder = config["PATHS"]["staging_folder"]
    today = datetime.now().strftime("%Y-%m-%d")

    staging_path = f"s3://{bucket}/{folder}/{today}/clean_data.csv"
    report_path = f"s3://{bucket}/{folder}/{today}/data_quality_report.csv"
    status_file = f"s3://{bucket}/file_status.csv"
    file_name = "clean_data.csv"

    # --- Ki·ªÉm tra clean_data.csv c√≥ P3 ch∆∞a ---
    logger.info("üîç ƒêang ki·ªÉm tra tr·∫°ng th√°i file tr∆∞·ªõc khi ch·∫°y Data Quality...")

    try:
        fs = s3fs.S3FileSystem(**MINIO_STORAGE_OPTIONS["client_kwargs"],
                               key=MINIO_STORAGE_OPTIONS["key"],
                               secret=MINIO_STORAGE_OPTIONS["secret"])

        if not fs.exists(status_file):
            logger.info("üî∏ Ch∆∞a c√≥ file_status.csv ‚Üí D·ª´ng Data Quality.")
            return

        df_status = pd.read_csv(status_file, storage_options=MINIO_STORAGE_OPTIONS)
        status = df_status.loc[df_status["file_name"] == file_name, "status"].values

        if len(status) == 0 or status[0] != "P3":
            logger.info(f"üî∏ {file_name} ch∆∞a P3 t·ª´ b∆∞·ªõc Load ‚Üí D·ª´ng Data Quality.")
            return

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ƒë·ªçc file_status.csv: {e}")
        return

    logger.info("‚úÖ File ƒë√£ ƒë·∫°t P3 ‚Üí B·∫Øt ƒë·∫ßu Data Quality")

    try:
        # === P1: b·∫Øt ƒë·∫ßu check ===
        update_file_status(file_name, "P1", bucket, MINIO_STORAGE_OPTIONS)
        logger.info(f"{file_name} - status: P1")

        df = pd.read_csv(staging_path, storage_options=MINIO_STORAGE_OPTIONS)
        logger.info(f"üì• ƒê√£ ƒë·ªçc {len(df)} d√≤ng staging")

        # === P2: ƒëang x·ª≠ l√Ω check ===
        update_file_status(file_name, "P2", bucket, MINIO_STORAGE_OPTIONS)
        logger.info(f"{file_name} - status: P2")

        # --- Check ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu ---
        report = {}
        report["num_rows"] = len(df)
        report["num_cols"] = len(df.columns)

        report["null_counts"] = df.isnull().sum().to_dict()
        report["duplicate_rows"] = df.duplicated().sum()

        if "gi√°" in df.columns:
            report["gi√°_min"] = df["gi√°"].min()
            report["gi√°_max"] = df["gi√°"].max()
            report["gi√°_invalid"] = df[df["gi√°"] <= 0].shape[0]

        # --- L∆∞u b√°o c√°o ---
        pd.DataFrame([report]).to_csv(report_path, index=False, storage_options=MINIO_STORAGE_OPTIONS)
        logger.info(f"üìÑ ƒê√£ l∆∞u b√°o c√°o v√†o: {report_path}")

        # === P3: ho√†n t·∫•t check ===
        update_file_status(file_name, "P3", bucket, MINIO_STORAGE_OPTIONS)
        logger.info(f"{file_name} - status: P3")

    except Exception as e:
        logger.error(f"‚ùå L·ªói Data Quality: {e}")

        # === P4: l·ªói ===
        update_file_status(file_name, "P4", bucket, MINIO_STORAGE_OPTIONS)
        logger.info(f"{file_name} - status: P4")

    # Upload log
    upload_log_to_minio(logger.log_file, step_name=step_name)


if __name__ == "__main__":
    main()
